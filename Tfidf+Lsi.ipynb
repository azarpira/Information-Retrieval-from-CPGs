{
 "cells": [
  {
   "cell_type": "raw",
   "id": "10d11f55-b6ad-4738-b4df-1ff898a5a6d6",
   "metadata": {},
   "source": [
    "This algorithme find efficiently the answer of a clincal question in a guideline text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cdb6d5d3-b7c3-4c90-a186-4e358b7ebafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(pdf):\n",
    "    from re import sub\n",
    "    t = ' '.join(pdf)\n",
    "    t = sub(r'(https.*\\s)' , '', t) \n",
    "    t = sub(r'●', ' ', t)\n",
    "    t = sub(r'⚬', '', t)\n",
    "    t = sub(r'\\n\\n', '|||', t)\n",
    "    t = sub(r'\\n', '|||',t)\n",
    "    t = sub(r'(\\d+/\\d+/\\d+)|(\\d+\\:\\d+)|(\\d+/\\d+)', ' ',t)\n",
    "    t = sub(r'(\\x0c)', '', t)\n",
    "    t_lines = t.split('|||')\n",
    "    t_lines = [line.strip() for line in t_lines if len(line.strip()) > 2]\n",
    "    for end, line in enumerate(t_lines):\n",
    "        if sum([(word in line) for word in ['Previous authors','Last reviewed','Guidelines and Resources']]) == True:\n",
    "            t_lines = t_lines[:end]\n",
    "            \n",
    "    #remove studies        \n",
    "    omit=[]\n",
    "    stop=0\n",
    "    for start, line in enumerate(t_lines):\n",
    "        if ('SUMMARY' in line) == True :\n",
    "            find=False\n",
    "            for stop, line in enumerate(t_lines[start:]):\n",
    "                if('Reference' in line) == True:\n",
    "                    omit += [i for i in range(start,start+stop+1)]\n",
    "                    find = True\n",
    "                    break\n",
    "            if not find:\n",
    "                omit+=[i for i in range(start,start+stop+1)]\n",
    "    \n",
    "    t_lines = [line for index, line in enumerate(t_lines) if index not in omit]\n",
    "    omit=[]\n",
    "    \n",
    "    #remove refrences\n",
    "    for num,line in enumerate(t_lines):\n",
    "        if sum([(word in line) for word in ['Latest change','Reference','Evidence-Based Medicine Guidelines',\n",
    "                                            'EBM Guidelines', 'Teuvo Tammela']]) == True:\n",
    "            omit += [num]\n",
    "    t_lines = [line for index, line in enumerate(t_lines) if index not in omit]\n",
    "    \n",
    "    #remove duplicate lines\n",
    "    t_lines = [ii for n,ii in enumerate(t_lines) if ii not in t_lines[:n]]   \n",
    "    \n",
    "    #paragraphs\n",
    "    headings = ['Complications', 'CLINICIAN', 'DynaMed Commentary', 'Prevention', 'Management',\n",
    " 'Also called', 'Evaluation', 'Overview', 'Testing', 'Differential', 'Diagnosis', 'Physical', 'History', 'Pathogenesis',\n",
    " 'Prognosis', 'Screening', 'Etiology', 'Pathogenesis', 'Risk', 'Epidemiology', 'Definitions', 'Description', 'Related Topics',\n",
    " 'Background', 'Definition', 'pathogenesis', 'Investigations', 'Investigation and treatment strategy', 'Investigation', 'Treatment',\n",
    "               'pathogenesis', 'Essentials', 'Epidemiology', 'Aetiology', 'Clinical signs', 'Laboratory', 'Symptoms', 'Grading',\n",
    "               'Drug', 'Principles', 'Pharmacologic prophylaxis', 'Nonpharmacologic prophylaxis', 'Recommendations',\n",
    "               'Medications', 'Indications', 'Antihypertensives', 'Antidepressants', 'Anticonvulsants', 'Antihistamines',\n",
    "               'Botulinum', 'Nutraceuticals', 'Riboflavin', 'Nonpharmacologic Therapies', 'Cognitive behavioral therapy',\n",
    "               'General', 'Primary investigations', 'Non-pharmacological', 'Grade', 'The most common adverse effects', \n",
    "               'Efficacy', 'Duration', 'Specific', 'Comparative', 'American', 'Transmission', 'aetiology', 'First aid',\n",
    "               'Workup', 'Conservative', 'Surgical', 'Prevalence', 'Factors', 'Referral', 'Antimicrobial', 'Prophylactic',\n",
    "               'Causative','Quality Improvement']\n",
    "    para=[]\n",
    "    for head in headings:\n",
    "        for num,line in enumerate(t_lines):\n",
    "            if (head in line) == True:\n",
    "                para+=[num]\n",
    "    para=sorted(list(set(para)))\n",
    "\n",
    "    paragraphs=[]\n",
    "    stop=0\n",
    "    paragraphs.append(' '.join(t_lines[:para[0]]))\n",
    "    for num,start in enumerate(para[:-1]):\n",
    "        stop = para[num+1]\n",
    "        paragraphs.append(' '.join(t_lines[start:stop]))\n",
    "    \n",
    "    paragraphs.append(' '.join(t_lines[stop:]))\n",
    "    \n",
    "    return(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4928ea27-2335-4121-8c22-a71d39a908c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#find the answer of question of expert in F1 and finding the correlation with the answer proposed par expert\n",
    "#besoin des paragraphs !!!!\n",
    "def find_answer(F1, question, suggestion):\n",
    "    import pdftotext\n",
    "    import numpy as np\n",
    "    from scipy.stats import pearsonr\n",
    "    from IPython.display import clear_output\n",
    "\n",
    "    \n",
    "    pdf_f = open(F1, 'rb')\n",
    "    pdf = pdftotext.PDF(pdf_f)\n",
    "    pdf_f.close()\n",
    "    \n",
    "    text1 = preprocess(pdf)\n",
    "    \n",
    "    from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, strip_non_alphanum, strip_numeric, strip_tags, strip_multiple_whitespaces, stem_text, strip_short\n",
    "    CUSTOM_FILTERS = [remove_stopwords, strip_non_alphanum, strip_tags, strip_multiple_whitespaces]\n",
    "    textClean1 = [preprocess_string(line, CUSTOM_FILTERS) for line in text1]\n",
    "    Q = preprocess_string(question, CUSTOM_FILTERS)\n",
    "    \n",
    "    common_texts = textClean1\n",
    "    \n",
    "    from gensim.models import RpModel, TfidfModel, LsiModel, LdaModel, HdpModel\n",
    "    from gensim.corpora import Dictionary\n",
    "    from gensim.test.utils import temporary_file\n",
    "    \n",
    "    dictionary = Dictionary(common_texts)  # fit dictionary\n",
    "    \n",
    "    corpus1 = [dictionary.doc2bow(text) for text in textClean1]  # convert texts to BoW format\n",
    "    \n",
    "    modelT = TfidfModel(corpus=corpus1, id2word=dictionary)\n",
    "    corpusT = modelT[corpus1]\n",
    "    d2bQ = dictionary.doc2bow(Q)\n",
    "    vecQT = modelT[d2bQ]\n",
    "\n",
    "\n",
    "    modelTL = LsiModel(corpus=corpusT)  # , id2word=dictionary  fit Lsi model over Tfidf\n",
    "    corpusTL = modelTL[corpusT]\n",
    "    vecQTL = modelTL[vecQT]\n",
    "\n",
    "    from gensim import similarities\n",
    "    index = similarities.MatrixSimilarity(corpusTL)  # transform corpus to LSI space and index it\n",
    "    \n",
    "    sims = index[vecQTL]  # perform a similarity query against the corpus\n",
    "    sorted_sims = sorted(list(enumerate(sims)), key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "    reponse=[]\n",
    "    s_answer=[]\n",
    "    \n",
    "    for doc_position, doc_score in sorted_sims:\n",
    "        if doc_score > 0.2:\n",
    "            reponse.append((doc_score, doc_position))\n",
    "            A = preprocess_string(suggestion, CUSTOM_FILTERS)\n",
    "            d2bA = dictionary.doc2bow(A)\n",
    "            vecATL = modelTL[modelT[d2bA]]\n",
    "            indexA = similarities.MatrixSimilarity(corpusTL)\n",
    "            simsA = indexA[vecATL]\n",
    "            s_answer.append(simsA[doc_position])\n",
    "  \n",
    "\n",
    "    algAnswer = [(text1[item[1]],item[0]) for item in reponse]\n",
    "    \n",
    "   \n",
    "    len_text1=[len(text) for text in text1]\n",
    "    len_ctext=[len(ctext) for ctext in common_texts]\n",
    "    \n",
    "    print(len_text1,'\\n\\n\\n',len_ctext)\n",
    "    \n",
    "    return(len_text1, len_ctext)\n",
    "    \n",
    "import glob\n",
    "from IPython.display import clear_output\n",
    "len_text=[]\n",
    "len_ctext=[]\n",
    "result=[]\n",
    "alg_answ=[]\n",
    "dir_name = glob.glob('PATH TO GUIDELINES IN PDF AND TEXT CONTAINING QUESTION AND ANSWERS IN TXT FORMAT')\n",
    "for item in dir_name:\n",
    "    for file in glob.glob(item):\n",
    "        name = file.split('/')[-1]\n",
    "        F1 = file+'/Dyall_'+name+'.pdf'\n",
    "        f = open(item+'/'+item.split('/')[-1]+'.txt', 'r', encoding='utf8', errors='ignore')\n",
    "        lines = f.readlines()\n",
    "        lines = [line.replace('\\n','') for line in lines]\n",
    "        f.close()\n",
    "        \n",
    "   \n",
    "        question = [lines[num+1] for num,line in enumerate(lines) if ('Clinical question' in line) == True][0][:-1]\n",
    "        suggestion = [' '.join(lines[num:]) for num,line in enumerate(lines) if ('conclu' in line.lower()) == True][0]\n",
    "        \n",
    "        answer = find_answer(F1, question, suggestion)\n",
    "        \n",
    "    len_text.append(answer[0])\n",
    "    len_ctext.append(answer[1])\n",
    "\n",
    "print('End...')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
